{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Code generation models are a type of generative model trained to generate code in a specific programming language based on natural language descriptions or code snippets. These models can assist developers by automating code writing, auto-completing code, or generating entire functions based on a description. I can leverage pre-trained models like GPT-2 or specialised models like Codex (a variant of GPT-3) to generate code from prompts."
      ],
      "metadata": {
        "id": "ZJqya4waUzQT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pW_2wc5ZUeDQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.Loading the pre-trained GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"  # GPT-2 model (can be replaced with Codex or larger models for better performance)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "model.eval()  # Set model to evaluation mode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZmmjdFKVxKk",
        "outputId": "99807e05-4841-46a6-9a8e-b685189854c6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.Generating code from a prompt (natural language)\n",
        "def generate_code(prompt, max_length=150, temperature=0.7, top_p=0.9):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "    #Generating code with the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1,\n",
        "                                 temperature=temperature, top_p=top_p, no_repeat_ngram_size=2)\n",
        "\n",
        "    code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return code\n",
        ""
      ],
      "metadata": {
        "id": "_g2BRChdWnxN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3.Example code prompt and generation\n",
        "prompt = \"Write a Python function that does statistical calulation\"\n",
        "generated_code = generate_code(prompt)\n",
        "\n",
        "print(\"Generated Code:\")\n",
        "print(generated_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7bz4UnFWviX",
        "outputId": "780ba81c-26ba-416e-f5d8-18e62ad293ed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Code:\n",
            "Write a Python function that does statistical calulation.\n",
            "\n",
            "import time import time.time import sys import os import random import numpy as np import matplotlib.pyplot as plt import pandas as pd import csv import json import from datetime import datetimes import date import sleep import logtime from time_tables import t = time . now () t . set_time ( 10 ) t. set ( 'time' , time )\n",
            ". plot ( t ) . plot_label ( \"Calculating the time series of the data\" ) # Plot the series to the right of t's time plot . append ( time ( ) ) plot = plot.plot ( data = data ) for t in plot\n"
          ]
        }
      ]
    }
  ]
}